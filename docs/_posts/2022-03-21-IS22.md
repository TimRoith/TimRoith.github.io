---
title:  "SIAM Conference on Imaging Science (IS22)"
---

Together with [Leon Bungert](https://sites.google.com/view/leon-bungert/news) and [Daniel Tenbrinck](https://www.math.fau.de/angewandte-mathematik-1/mitarbeiter/dr-daniel-tenbrinck/) we organized a three-part Minisymposium for the [SIAM conference IS22](https://www.siam.org/conferences/cm/conference/is22) on Recent Advances on Stable Neural Networks.

## Abstract

Modern machine learning methods, in particular deep learning approaches, have enjoyed unparalleled success in a variety of challenging application fields, like image recognition and medical image reconstruction. While previous research focused on achieving high accuracy and good generalization ability, the consensus has emerged that other properties like stability and robustness are of equal importance. This has especially been supported by instabilities of neural networks, revealed through adversarial attacks. These are intentionally and imperceptibly distorted images that fool the network and lead to misclassification.

In this two-part minisymposium we gather researchers from both mathematics and machine learning that have been driving the research in this field. We aim at sparking new collaborations in this vibrant intersection and offering a platform for scientific exchange. The covered topics include regularization methods for training, as well as the design of inherently stable network architectures.

## Program

| Speaker       | Title |
| --------      | ----- |
|  Tim Roith    | Regularize the Adversary: The Role of the Lipschitz Constant in Modern Machine Learning |
| Tom Goldstein | Think Deeper, Generalize Better: Solving Logical Reasoning Problems with Recurrent Thinking Systems |
| Leon Bungert  | A New Perspective on Adversarial Training as Nonlocal Geometric Problem |
| Mark Sellke   | A Universal Law of Robustness via Isoperimetry |
| Jan Stanczuk  | Wasserstein GANS Work Because They Fail (to Approximate the Wasserstein Distance) |
