---
title:  "Uniform Convergence Rates for Lipschitz Learning on Graphs"
categories: paper
---

I am very happy to announce that our paper on
[Uniform Convergence Rates for Lipschitz Learning on Graphs](https://academic.oup.com/imajna/advance-article/doi/10.1093/imanum/drac048/6705556) has appeared in the IMA journal of numerical analysis.

The code for the experiments can be found on our GitHub repository [LipschitzLearningRates](https://github.com/jwcalder/LipschitzLearningRates).

<img src="/assets/img/neumann_star.png" width="900">

<blockquote class="twitter-tweet" data-lang="en">
<a href="https://twitter.com/tim_roith/status/1574733885623390209">
</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

## Lecture on Lipschitz Learning Convergence Rate

During the workshop [Dynamics and Discretization: PDEs, Sampling, and Optimization](https://simons.berkeley.edu/workshops/gmos2021-2) in October 2021 at the Simons Insitute Berkeley Jeff Calder gave a lecture about the subject. The video is available on YouTube.

<iframe width="420" height="315" src="https://www.youtube.com/embed/UGU_f9U_M28" frameborder="0"></iframe>